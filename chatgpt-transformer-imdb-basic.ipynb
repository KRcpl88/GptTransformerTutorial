{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Tutorial Generated by GPT 4\r\n",
        "\r\n",
        "Both the descriptive explanations and the code samples for this tutorial were generated entirely with chatGPT using the GPT 4 model. In some cases the initial code had minor errors, these errors were also fixed by GPT 4 by feeding the errors back into GPT 4 and GPT 4 would generate new code.  This debugging process was repeated at most 3 times.  The last example, with multi-headed attention and token and position embedding, was the most complicated and took GPT 4 3 iterations to get it right.\r\n",
        "\r\n",
        "This is an basic tutorial which uses built in layers from Tensorflow for the self attention mechanism and token and position embedding\r\n",
        "\r\n",
        "## IMDB Sentiment Analysis\r\n",
        "\r\n",
        "The Keras IMDB dataset is a popular dataset for sentiment analysis tasks in natural language processing (NLP). It contains 50,000 movie reviews from the Internet Movie Database (IMDB) labeled as either positive (1) or negative (0) based on the sentiment expressed in the review. The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing.\r\n",
        "\r\n",
        "The reviews in the dataset have been preprocessed, and each review is encoded as a sequence of word indices (integers). The indices represent the overall frequency rank of the words in the entire dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. This encoding allows for faster processing and less memory usage compared to working with raw text data.\r\n",
        "\r\n",
        "The Keras IMDB dataset is typically used for binary classification tasks, where the goal is to build a machine learning model that can predict whether a given movie review is positive or negative based on the text content. The dataset is accessible through the tensorflow.keras.datasets module in the TensorFlow library."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, Embedding, GlobalAveragePooling1D, Add\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.losses import BinaryCrossentropy\r\n",
        "from keras.datasets import imdb"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-05-09 19:20:22.644788: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-05-09 19:20:22.694539: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-05-09 19:20:22.695518: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-09 19:20:23.586942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683660024651
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single headed attention \r\n",
        "This code will create a single-headed attention transformer model and train it on the IMDB dataset. The model has an input layer, an embedding layer, a layer normalization, a multi-head attention layer with a single head, another layer normalization, and finally a dense layer with a softmax activation function. The model is compiled with the SparseCategoricalCrossentropy loss function and the Adam optimizer. It is then trained for 10 epochs and evaluated on the test set.\r\n",
        "\r\n",
        "\r\n",
        "Since we are working on a classification task, we should not output a probability distribution over the entire vocabulary. Instead, we should output a single probability for each class."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\r\n",
        "vocab_size = 10000\r\n",
        "max_length = 200\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\r\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\r\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\r\n",
        "\r\n",
        "# Define the single-headed attention transformer model\r\n",
        "def transformer_model(vocab_size, d_model, input_length):\r\n",
        "    inputs = Input(shape=(input_length,))\r\n",
        "    embeddings = Embedding(vocab_size, d_model)(inputs)\r\n",
        "\r\n",
        "    normalized_embeddings = LayerNormalization(epsilon=1e-6)(embeddings)\r\n",
        "    attention = MultiHeadAttention(num_heads=1, key_dim=d_model)(normalized_embeddings, normalized_embeddings)\r\n",
        "    attention = Dropout(0.1)(attention)\r\n",
        "    attention = LayerNormalization(epsilon=1e-6)(attention + normalized_embeddings)\r\n",
        "\r\n",
        "    pooled = GlobalAveragePooling1D()(attention)\r\n",
        "    outputs = Dense(1, activation='sigmoid')(pooled)\r\n",
        "    model = Model(inputs=inputs, outputs=outputs)\r\n",
        "    return model\r\n",
        "\r\n",
        "# Create and compile the model\r\n",
        "d_model = 128\r\n",
        "model = transformer_model(vocab_size, d_model, max_length)\r\n",
        "model.compile(loss=BinaryCrossentropy(from_logits=False), optimizer=Adam(), metrics=['accuracy'])\r\n",
        "\r\n",
        "# Train the model\r\n",
        "batch_size = 64\r\n",
        "epochs = 6\r\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\r\n",
        "\r\n",
        "# Evaluate the model\r\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\r\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-05-09 19:20:29.082765: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/6\n313/313 [==============================] - 35s 108ms/step - loss: 0.3421 - accuracy: 0.8522 - val_loss: 0.2927 - val_accuracy: 0.8814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/6\n313/313 [==============================] - 34s 107ms/step - loss: 0.1561 - accuracy: 0.9419 - val_loss: 0.3370 - val_accuracy: 0.8734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/6\n313/313 [==============================] - 34s 108ms/step - loss: 0.0790 - accuracy: 0.9732 - val_loss: 0.3894 - val_accuracy: 0.8700\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/6\n313/313 [==============================] - 33s 104ms/step - loss: 0.0388 - accuracy: 0.9862 - val_loss: 0.5137 - val_accuracy: 0.8702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/6\n313/313 [==============================] - 33s 104ms/step - loss: 0.0240 - accuracy: 0.9912 - val_loss: 0.6874 - val_accuracy: 0.8588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/6\n313/313 [==============================] - 34s 107ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.8411 - val_accuracy: 0.8544\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n782/782 [==============================] - 14s 18ms/step - loss: 1.0262 - accuracy: 0.8235\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nTest Loss: 1.0261664390563965, Test Accuracy: 0.8234800100326538\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683660245220
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Headed attention\r\n",
        "\r\n",
        "This code creates a multi-headed attention transformer model and trains it on the IMDB dataset. The model has an input layer, an embedding layer, a layer normalization, a multi-head attention layer, another layer normalization, a global average pooling layer, and finally a dense layer with a sigmoid activation function. The model is compiled with the BinaryCrossentropy loss function and the Adam optimizer. It is then trained for 10 epochs and evaluated on the test set."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\r\n",
        "vocab_size = 20000\r\n",
        "max_length = 200\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\r\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\r\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\r\n",
        "\r\n",
        "# Define the multi-headed attention transformer model\r\n",
        "def transformer_model(vocab_size, d_model, input_length, num_heads):\r\n",
        "    inputs = Input(shape=(input_length,))\r\n",
        "    embeddings = Embedding(vocab_size, d_model)(inputs)\r\n",
        "\r\n",
        "    normalized_embeddings = LayerNormalization(epsilon=1e-6)(embeddings)\r\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(normalized_embeddings, normalized_embeddings)\r\n",
        "    attention = Dropout(0.1)(attention)\r\n",
        "    attention = LayerNormalization(epsilon=1e-6)(attention + normalized_embeddings)\r\n",
        "\r\n",
        "    pooled = GlobalAveragePooling1D()(attention)\r\n",
        "    outputs = Dense(1, activation='sigmoid')(pooled)\r\n",
        "    model = Model(inputs=inputs, outputs=outputs)\r\n",
        "    return model\r\n",
        "\r\n",
        "# Create and compile the model\r\n",
        "d_model = 128\r\n",
        "num_heads = 8\r\n",
        "model = transformer_model(vocab_size, d_model, max_length, num_heads)\r\n",
        "model.compile(loss=BinaryCrossentropy(from_logits=False), optimizer=Adam(), metrics=['accuracy'])\r\n",
        "\r\n",
        "# Train the model\r\n",
        "batch_size = 64\r\n",
        "epochs = 6\r\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\r\n",
        "\r\n",
        "# Evaluate the model\r\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\r\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/6\n313/313 [==============================] - 239s 761ms/step - loss: 0.3516 - accuracy: 0.8425 - val_loss: 0.2883 - val_accuracy: 0.8772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/6\n313/313 [==============================] - 238s 759ms/step - loss: 0.1575 - accuracy: 0.9395 - val_loss: 0.3375 - val_accuracy: 0.8748\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/6\n313/313 [==============================] - 238s 760ms/step - loss: 0.0847 - accuracy: 0.9707 - val_loss: 0.4205 - val_accuracy: 0.8674\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/6\n313/313 [==============================] - 237s 759ms/step - loss: 0.0376 - accuracy: 0.9862 - val_loss: 0.6573 - val_accuracy: 0.8626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/6\n313/313 [==============================] - 237s 756ms/step - loss: 0.0234 - accuracy: 0.9915 - val_loss: 0.6470 - val_accuracy: 0.8636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/6\n313/313 [==============================] - 232s 743ms/step - loss: 0.0184 - accuracy: 0.9942 - val_loss: 0.7200 - val_accuracy: 0.8580\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n782/782 [==============================] - 102s 131ms/step - loss: 0.8519 - accuracy: 0.8349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nTest Loss: 0.8519366383552551, Test Accuracy: 0.834879994392395\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683661773691
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Headed Attention with Token and Position Embedding\r\n",
        "\r\n",
        "_Generate python code using the tensorflow module to build and train a transformer model with a multi headed attention mechanism and token and position embedding, using keras.layers.MultiHeadAttention_\r\n",
        "\r\n",
        "This code creates a multi-headed attention transformer model with token and position embeddings and trains it on the IMDB dataset. The model has an input layer, an embedding layer, an added position encoding, a layer normalization, a multi-head attention layer, another layer normalization, a global average pooling layer, and finally a dense layer with a sigmoid activation function. The model is compiled with the BinaryCrossentropy loss function and the Adam optimizer. It is then trained for 10 epochs and evaluated on the test set."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\r\n",
        "vocab_size = 20000\r\n",
        "max_length = 200\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\r\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\r\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\r\n",
        "\r\n",
        "# Positional encoding\r\n",
        "def positional_encoding(position, d_model):\r\n",
        "    angle_rads = np.arange(0, d_model, 2) / np.array([10000 ** (i / d_model) for i in range(0, d_model, 2)])\r\n",
        "    angle_rads = np.repeat(np.arange(position)[:, np.newaxis], d_model // 2, axis=1) * angle_rads\r\n",
        "\r\n",
        "    pos_encoding = np.zeros((1, position, d_model))\r\n",
        "    pos_encoding[:, :, 0::2] = np.sin(angle_rads)\r\n",
        "    pos_encoding[:, :, 1::2] = np.cos(angle_rads)\r\n",
        "    \r\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\r\n",
        "\r\n",
        "# Define the multi-headed attention transformer model with token and position embeddings\r\n",
        "def transformer_model(vocab_size, d_model, input_length, num_heads):\r\n",
        "    inputs = Input(shape=(input_length,))\r\n",
        "    token_embeddings = Embedding(vocab_size, d_model)(inputs)\r\n",
        "\r\n",
        "    position_embeddings = positional_encoding(input_length, d_model)\r\n",
        "    embeddings = Add()([token_embeddings, position_embeddings])\r\n",
        "\r\n",
        "    normalized_embeddings = LayerNormalization(epsilon=1e-6)(embeddings)\r\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(normalized_embeddings, normalized_embeddings)\r\n",
        "    attention = Dropout(0.1)(attention)\r\n",
        "    attention = LayerNormalization(epsilon=1e-6)(attention + normalized_embeddings)\r\n",
        "\r\n",
        "    pooled = GlobalAveragePooling1D()(attention)\r\n",
        "    outputs = Dense(1, activation='sigmoid')(pooled)\r\n",
        "    model = Model(inputs=inputs, outputs=outputs)\r\n",
        "    return model\r\n",
        "\r\n",
        "# Create and compile the model\r\n",
        "d_model = 128\r\n",
        "num_heads = 8\r\n",
        "model = transformer_model(vocab_size, d_model, max_length, num_heads)\r\n",
        "model.compile(loss=BinaryCrossentropy(from_logits=False), optimizer=Adam(), metrics=['accuracy'])\r\n",
        "\r\n",
        "# Train the model\r\n",
        "batch_size = 64\r\n",
        "epochs = 6\r\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\r\n",
        "\r\n",
        "# Evaluate the model\r\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\r\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/6\n313/313 [==============================] - 233s 740ms/step - loss: 0.4260 - accuracy: 0.7734 - val_loss: 0.2810 - val_accuracy: 0.8876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/6\n313/313 [==============================] - 233s 743ms/step - loss: 0.1880 - accuracy: 0.9276 - val_loss: 0.2898 - val_accuracy: 0.8864\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/6\n313/313 [==============================] - 233s 744ms/step - loss: 0.1182 - accuracy: 0.9564 - val_loss: 0.3311 - val_accuracy: 0.8780\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/6\n313/313 [==============================] - 232s 743ms/step - loss: 0.0672 - accuracy: 0.9773 - val_loss: 0.4549 - val_accuracy: 0.8732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/6\n313/313 [==============================] - 233s 745ms/step - loss: 0.0329 - accuracy: 0.9886 - val_loss: 0.6167 - val_accuracy: 0.8656\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/6\n313/313 [==============================] - 234s 746ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 0.9039 - val_accuracy: 0.8506\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n782/782 [==============================] - 103s 131ms/step - loss: 0.9521 - accuracy: 0.8419\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nTest Loss: 0.9520815014839172, Test Accuracy: 0.8418800234794617\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683663277863
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}