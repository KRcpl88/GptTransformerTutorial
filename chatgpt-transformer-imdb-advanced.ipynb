{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Transformer Tutorial Generated by GPT 4\r\n",
        "\r\n",
        "Both the descriptive explanations and the code samples for this tutorial were generated entirely with chatGPT using the GPT 4 model. In some cases the initial code had minor errors, these errors were also fixed by GPT 4 by feeding the errors back into GPT 4 and GPT 4 would generate new code.\r\n",
        "\r\n",
        "This is an advanced tutorial which builds the main components of the Transformer model, the multi headed attention mechanism and the position and token embedding, from scratch in PyTorch.\r\n",
        "\r\n",
        "## IMDB Sentiment Analysis\r\n",
        "\r\n",
        "The Keras IMDB dataset is a popular dataset for sentiment analysis tasks in natural language processing (NLP). It contains 50,000 movie reviews from the Internet Movie Database (IMDB) labeled as either positive (1) or negative (0) based on the sentiment expressed in the review. The dataset is divided into 25,000 reviews for training and 25,000 reviews for testing.\r\n",
        "\r\n",
        "The reviews in the dataset have been preprocessed, and each review is encoded as a sequence of word indices (integers). The indices represent the overall frequency rank of the words in the entire dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. This encoding allows for faster processing and less memory usage compared to working with raw text data.\r\n",
        "\r\n",
        "The Keras IMDB dataset is typically used for binary classification tasks, where the goal is to build a machine learning model that can predict whether a given movie review is positive or negative based on the text content. The dataset is accessible through the tensorflow.keras.datasets module in the TensorFlow library.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "from tensorflow.keras.datasets import imdb\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-05-10 00:42:34.438071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-10 00:42:35.407668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683679356272
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Headed attention\r\n",
        "\r\n",
        "This class takes as input the model dimension d_model and the number of attention heads num_heads. The forward method takes a tensor of shape (batch_size, sequence_length, d_model) and an optional mask, and it outputs the context vectors and attention weights."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\r\n",
        "    def __init__(self, d_model, num_heads):\r\n",
        "        super(MultiHeadSelfAttention, self).__init__()\r\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\r\n",
        "\r\n",
        "        self.d_model = d_model\r\n",
        "        self.num_heads = num_heads\r\n",
        "        self.head_dim = d_model // num_heads\r\n",
        "\r\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\r\n",
        "        self.W_K = nn.Linear(d_model, d_model)\r\n",
        "        self.W_V = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "        self.fc = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\r\n",
        "        attention_logits = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\r\n",
        "        if mask is not None:\r\n",
        "            attention_logits = attention_logits.masked_fill(mask == 0, float('-inf'))\r\n",
        "        attention_weights = F.softmax(attention_logits, dim=-1)\r\n",
        "        return torch.matmul(attention_weights, V), attention_weights\r\n",
        "\r\n",
        "    def split_heads(self, x):\r\n",
        "        batch_size, seq_len, _ = x.size()\r\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n",
        "\r\n",
        "    def combine_heads(self, x):\r\n",
        "        batch_size, _, seq_len, _ = x.size()\r\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\r\n",
        "\r\n",
        "    def forward(self, x, mask=None):\r\n",
        "        batch_size, seq_len, _ = x.size()\r\n",
        "\r\n",
        "        Q = self.split_heads(self.W_Q(x))\r\n",
        "        K = self.split_heads(self.W_K(x))\r\n",
        "        V = self.split_heads(self.W_V(x))\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            mask = mask.unsqueeze(1)\r\n",
        "\r\n",
        "        context_vectors, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\r\n",
        "        context_vectors = self.combine_heads(context_vectors)\r\n",
        "\r\n",
        "        return self.fc(context_vectors), attention_weights\r\n",
        "\r\n",
        "# Example usage:\r\n",
        "input_tensor = torch.rand(16, 50, d_model)  # 16 is batch_size and 50 is sequence length\r\n",
        "\r\n",
        "self_attention = MultiHeadSelfAttention(d_model, num_heads)\r\n",
        "output, attention_weights = self_attention(input_tensor)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683679356770
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token and Position Embedding\r\n",
        "\r\n",
        "This class takes as input the vocabulary size vocab_size, the model dimension d_model, and the maximum sequence length max_seq_len. The forward method takes a tensor of shape (batch_size, sequence_length) with token ids and outputs the combined token and position embeddings with shape (batch_size, sequence_length, d_model)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenPositionEmbedding(nn.Module):\r\n",
        "    def __init__(self, vocab_size, d_model, max_seq_len):\r\n",
        "        super(TokenPositionEmbedding, self).__init__()\r\n",
        "\r\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(0.1)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        batch_size, seq_len = x.size()\r\n",
        "\r\n",
        "        # Create the position ids from 0 to max_seq_len - 1\r\n",
        "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\r\n",
        "\r\n",
        "        # Get token and position embeddings\r\n",
        "        token_embeds = self.token_embedding(x)\r\n",
        "        position_embeds = self.position_embedding(position_ids)\r\n",
        "\r\n",
        "        # Combine token and position embeddings\r\n",
        "        embeddings = token_embeds + position_embeds\r\n",
        "\r\n",
        "        return self.dropout(embeddings)\r\n",
        "\r\n",
        "# Example usage:\r\n",
        "vocab_size = 20000\r\n",
        "max_seq_len = 200\r\n",
        "input_ids = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\r\n",
        "\r\n",
        "embedding_layer = TokenPositionEmbedding(vocab_size, d_model, max_seq_len)\r\n",
        "embeddings = embedding_layer(input_ids)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683679357045
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfomer Block\r\n",
        "This class takes as input the model dimension d_model, the number of attention heads num_heads, the feed-forward hidden dimension d_ff, the vocabulary size vocab_size, and the maximum sequence length max_seq_len. The forward method takes a tensor of shape (batch_size, sequence_length) with token ids and an optional mask, and it outputs the processed tensor with shape (batch_size, sequence_length, d_model)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\r\n",
        "    def __init__(self, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout=0.1):\r\n",
        "        super(TransformerBlock, self).__init__()\r\n",
        "\r\n",
        "        self.embedding_layer = TokenPositionEmbedding(vocab_size, d_model, max_seq_len)\r\n",
        "\r\n",
        "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\r\n",
        "        self.norm1 = nn.LayerNorm(d_model)\r\n",
        "        self.dropout1 = nn.Dropout(dropout)\r\n",
        "\r\n",
        "        self.feed_forward = nn.Sequential(\r\n",
        "            nn.Linear(d_model, d_ff),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(d_ff, d_model)\r\n",
        "        )\r\n",
        "        self.norm2 = nn.LayerNorm(d_model)\r\n",
        "        self.dropout2 = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x, mask=None):\r\n",
        "        # Token and position embedding\r\n",
        "        x = self.embedding_layer(x)\r\n",
        "\r\n",
        "        # Multi-head self-attention\r\n",
        "        attn_output, _ = self.self_attention(x, mask)\r\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\r\n",
        "\r\n",
        "        # Position-wise feed-forward\r\n",
        "        ff_output = self.feed_forward(x)\r\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "# Example usage:\r\n",
        "\r\n",
        "\r\n",
        "input_ids = torch.randint(0, vocab_size, (16, max_seq_len))  # 16 is batch_size\r\n",
        "\r\n",
        "transformer_block = TransformerBlock(d_model, num_heads, d_ff, vocab_size, max_seq_len)\r\n",
        "output = transformer_block(input_ids)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683679357227
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the IMDB Data Set\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\r\n",
        "    def __init__(self, x, y):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.x)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return torch.tensor(self.x[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.float)\r\n",
        "\r\n",
        "def load_imdb_data(num_words, max_seq_len):\r\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\r\n",
        "\r\n",
        "    # Pad sequences to max_seq_len\r\n",
        "    x_train = pad_sequences(x_train, maxlen=max_seq_len, padding='post', truncating='post')\r\n",
        "    x_test = pad_sequences(x_test, maxlen=max_seq_len, padding='post', truncating='post')\r\n",
        "\r\n",
        "    return x_train, y_train, x_test, y_test\r\n",
        "\r\n",
        "# Example usage:\r\n",
        "num_words = vocab_size\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "x_train, y_train, x_test, y_test = load_imdb_data(num_words, max_seq_len)\r\n",
        "\r\n",
        "train_dataset = IMDBDataset(x_train, y_train)\r\n",
        "test_dataset = IMDBDataset(x_test, y_test)\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683679360748
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Train the Model\r\n",
        "\r\n",
        "Here's an example of building and training a transformer model using TransformerBlock, MultiHeadSelfAttention, TokenAndPositionEmbedding, and IMDBDataset from the previous examples. This example calculates and outputs the loss and accuracy for both training and test data for each epoch:\r\n",
        "\r\n",
        "This example creates a TransformerClassifier class that uses the TransformerBlock as the main component. The output of the transformer block is pooled along the sequence dimension using mean pooling before passing through a linear layer for classification.\r\n",
        "\r\n",
        "The training loop iterates through num_epochs and calculates the training and test loss and accuracy for each epoch. Note that the model should be set to train mode during training and eval mode during evaluation to enable/disable dropout and other regularization techniques correctly.\r\n",
        "\r\n",
        "The main components of the code are as follows:\r\n",
        "\r\n",
        "Loading the IMDB dataset: The load_imdb_data function is called to load the IMDB dataset, preprocess it by padding or truncating sequences to a fixed length (max_seq_len), and split it into training and testing sets.\r\n",
        "\r\n",
        "Creating Dataset and DataLoader instances: PyTorch Dataset and DataLoader instances are created for the training and validation sets. These will be used to iterate through the data during the training process.\r\n",
        "\r\n",
        "Defining the model: The TransformerClassifier class is created by combining the TransformerBlock with a fully connected layer for classification. This class is then instantiated using the hyperparameters, such as d_model, num_heads, and d_ff.\r\n",
        "\r\n",
        "Setting up the training loop: The model is trained for a specified number of epochs using the CrossEntropyLoss and the Adam optimizer. For each epoch, the model is trained on the training set and evaluated on the validation set. The loss and accuracy for both training and validation sets are calculated and printed for each epoch.\r\n",
        "\r\n",
        "In summary, this sample code demonstrates how to build, train, and evaluate a simple Transformer-based model for sentiment analysis on the Keras IMDB dataset. The model is trained using a single TransformerBlock and the performance metrics (loss and accuracy) are reported for each epoch.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\r\n",
        "    def __init__(self, d_model, num_heads, d_ff, vocab_size, max_seq_len, num_classes, dropout=0.1):\r\n",
        "        super(TransformerClassifier, self).__init__()\r\n",
        "\r\n",
        "        self.transformer_block = TransformerBlock(d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout)\r\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\r\n",
        "\r\n",
        "    def forward(self, x, mask=None):\r\n",
        "        x = self.transformer_block(x, mask)\r\n",
        "        x = x.mean(dim=1)\r\n",
        "        return self.classifier(x)\r\n",
        "\r\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\r\n",
        "    model.train()\r\n",
        "    running_loss = 0.0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    for inputs, labels in loader:\r\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        outputs = model(inputs)\r\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        running_loss += loss.item()\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += ((outputs > 0) == labels.unsqueeze(1)).sum().item()\r\n",
        "\r\n",
        "    return running_loss / len(loader), correct / total\r\n",
        "\r\n",
        "def evaluate(model, loader, criterion, device):\r\n",
        "    model.eval()\r\n",
        "    running_loss = 0.0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for inputs, labels in loader:\r\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "\r\n",
        "            outputs = model(inputs)\r\n",
        "            loss = criterion(outputs, labels.unsqueeze(1))\r\n",
        "\r\n",
        "            running_loss += loss.item()\r\n",
        "            total += labels.size(0)\r\n",
        "            correct += ((outputs > 0) == labels.unsqueeze(1)).sum().item()\r\n",
        "\r\n",
        "    return running_loss / len(loader), correct / total\r\n",
        "\r\n",
        "# Model and training parameters\r\n",
        "num_classes = 1\r\n",
        "dropout = 0.1\r\n",
        "num_epochs = 10\r\n",
        "lr = 1e-4\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "# Load data and create DataLoaders\r\n",
        "x_train, y_train, x_test, y_test = load_imdb_data(num_words, max_seq_len)\r\n",
        "train_dataset = IMDBDataset(x_train, y_train)\r\n",
        "test_dataset = IMDBDataset(x_test, y_test)\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\r\n",
        "\r\n",
        "# Create and train the model\r\n",
        "model = TransformerClassifier(d_model, num_heads, d_ff, vocab_size, max_seq_len, num_classes, dropout).to(device)\r\n",
        "criterion = nn.BCEWithLogitsLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\r\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\r\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\r\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\r\n",
        "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/10, Train Loss: 0.6343, Train Acc: 0.6237, Test Loss: 0.5478, Test Accuracy: 0.7199\nEpoch 2/10, Train Loss: 0.5074, Train Acc: 0.7491, Test Loss: 0.4775, Test Accuracy: 0.7671\nEpoch 3/10, Train Loss: 0.4477, Train Acc: 0.7916, Test Loss: 0.4384, Test Accuracy: 0.7941\nEpoch 4/10, Train Loss: 0.4066, Train Acc: 0.8114, Test Loss: 0.4271, Test Accuracy: 0.8021\nEpoch 5/10, Train Loss: 0.3753, Train Acc: 0.8319, Test Loss: 0.4086, Test Accuracy: 0.8119\nEpoch 6/10, Train Loss: 0.3532, Train Acc: 0.8428, Test Loss: 0.4042, Test Accuracy: 0.8186\nEpoch 7/10, Train Loss: 0.3323, Train Acc: 0.8556, Test Loss: 0.3939, Test Accuracy: 0.8240\nEpoch 8/10, Train Loss: 0.3138, Train Acc: 0.8660, Test Loss: 0.3914, Test Accuracy: 0.8277\nEpoch 9/10, Train Loss: 0.2999, Train Acc: 0.8724, Test Loss: 0.3866, Test Accuracy: 0.8320\nEpoch 10/10, Train Loss: 0.2818, Train Acc: 0.8821, Test Loss: 0.3864, Test Accuracy: 0.8324\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1683679883373
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
